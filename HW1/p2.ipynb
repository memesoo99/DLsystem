{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def true_function(x):\n",
    "    return x + np.sin(1.5 * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P1.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "np.random.seed(42) \n",
    "x_values = np.linspace(0, 10, 20) # Create 20 data from 0 to 10\n",
    "\n",
    "# True function\n",
    "f_x = x_values + np.sin(1.5 * x_values)\n",
    "\n",
    "# Generate noise from N(0, 0.3)\n",
    "epsilon = np.random.normal(0, 0.3, 20) # Create noise of N(0,0.3)\n",
    "\n",
    "# Generate y = f(x) + epsilon\n",
    "y_values = f_x + epsilon\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_values, y_values, label='Generated Data (y)', color='blue', alpha=0.7)\n",
    "plt.plot(x_values, f_x, label='True Function f(x)', color='red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Generated Dataset and True Function')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "np.random.seed(42)  # For reproducibility\n",
    "size = 20  # Dataset size\n",
    "x_values = np.linspace(0, 10, size)\n",
    "noise_std = 0.3\n",
    "\n",
    "f_x = x_values + np.sin(1.5 * x_values)\n",
    "\n",
    "# Generate noise from N(0, 0.3)\n",
    "epsilon = np.random.normal(0, noise_std, size)\n",
    "y_values = f_x + epsilon\n",
    "x_reshaped = x_values.reshape(-1, 1)\n",
    "\n",
    "degrees = [1, 3, 10]\n",
    "estimators = []\n",
    "\n",
    "for degree in degrees:\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_poly = poly.fit_transform(x_reshaped)\n",
    "    \n",
    "    # Linear regression\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y_values)\n",
    "    \n",
    "    y_pred = model.predict(X_poly)\n",
    "    estimators.append((degree, y_pred))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_values, y_values, label='Sampled Data', color='black', alpha=0.7)\n",
    "plt.plot(x_values, f_x, label='f(x)', color='red', linewidth=2)\n",
    "\n",
    "colors = ['green', 'pink', 'purple']\n",
    "for (degree, y_pred), color in zip(estimators, colors):\n",
    "    plt.plot(x_values, y_pred, label=f'Estimator g{degree}(x)', color=color)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('True Function and Polynomial Estimators')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P1.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_datasets = 100\n",
    "dataset_size = 50\n",
    "train_size = int(0.8 * dataset_size)\n",
    "max_degree = 15\n",
    "noise_std = 0.3\n",
    "test_size = 10 \n",
    "train_size = dataset_size - test_size  \n",
    "\n",
    "x_test_fixed = np.linspace(0, 10, test_size)\n",
    "epsilon_test = np.random.normal(0, noise_std, test_size)\n",
    "y_test_fixed = true_function(x_test_fixed) + epsilon_test\n",
    "\n",
    "bias_squared = []\n",
    "variance = []\n",
    "errors = []\n",
    "\n",
    "for degree in range(1, max_degree + 1):\n",
    "    predictions_on_test = []\n",
    "\n",
    "    for _ in range(num_datasets): # MAke 100 diff dataset randomly\n",
    "        x_train = np.random.choice(np.linspace(0, 10, 1000), train_size, replace=False)\n",
    "        epsilon_train = np.random.normal(0, noise_std, train_size)\n",
    "        y_train = true_function(x_train) + epsilon_train\n",
    "\n",
    "        x_train_reshaped = x_train.reshape(-1, 1)\n",
    "        x_test_reshaped = x_test_fixed.reshape(-1, 1)\n",
    "        \n",
    "        # Polynomial transformation\n",
    "        poly = PolynomialFeatures(degree)\n",
    "        X_train_poly = poly.fit_transform(x_train_reshaped)\n",
    "        X_test_poly = poly.transform(x_test_reshaped)\n",
    "        \n",
    "        # Fit model and make predictions on fixed test set\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train_poly, y_train)\n",
    "        y_pred = model.predict(X_test_poly)\n",
    "        \n",
    "        predictions_on_test.append(y_pred)\n",
    "\n",
    "    predictions_on_test = np.array(predictions_on_test)\n",
    "    mean_prediction = np.mean(predictions_on_test, axis=0)\n",
    "    squared_bias = np.mean((mean_prediction - y_test_fixed) ** 2)\n",
    "    var = np.mean(np.var(predictions_on_test, axis=0))\n",
    "    mse = squared_bias + var + noise_std ** 2  # Including the noise variance\n",
    "\n",
    "    bias_squared.append(squared_bias)\n",
    "    variance.append(var)\n",
    "    errors.append(mse)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, max_degree + 1), bias_squared, label='Squared Bias', color='blue', marker='o')\n",
    "plt.plot(range(1, max_degree + 1), variance, label='Variance', color='green', marker='o')\n",
    "plt.plot(range(1, max_degree + 1), errors, label='MSE Error', color='red', marker='o')\n",
    "plt.xlabel('Model Complexity gi(x)')\n",
    "plt.ylabel('Value (Log Scale)')\n",
    "plt.title('Bias-Variance Tradeoff with Model Complexity (Log Scale)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "regularization_rate = 1.0\n",
    "\n",
    "bias_squared_regularized = []\n",
    "variance_regularized = []\n",
    "errors_regularized = []\n",
    "degree = 10\n",
    "predictions_on_test_unregularized = []\n",
    "predictions_on_test_regularized = []\n",
    "\n",
    "for _ in range(num_datasets):\n",
    "    # Generate random dataset\n",
    "    x_values = np.linspace(0, 10, dataset_size)\n",
    "    epsilon = np.random.normal(0, noise_std, dataset_size)\n",
    "    y_values = true_function(x_values) + epsilon\n",
    "    \n",
    "    x_train, x_test = x_values[:train_size], x_values[train_size:]\n",
    "    y_train, y_test = y_values[:train_size], y_values[train_size:]\n",
    "    x_train_reshaped = x_train.reshape(-1, 1)\n",
    "    x_test_reshaped = x_test.reshape(-1, 1)\n",
    "    \n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_train_poly = poly.fit_transform(x_train_reshaped)\n",
    "    X_test_poly = poly.transform(x_test_reshaped)\n",
    "    \n",
    "    model_unregularized = LinearRegression()\n",
    "    model_unregularized.fit(X_train_poly, y_train)\n",
    "    y_pred_unregularized = model_unregularized.predict(X_test_poly)\n",
    "    predictions_on_test_unregularized.append(y_pred_unregularized)\n",
    "    \n",
    "    model_regularized = Ridge(alpha=regularization_rate)\n",
    "    model_regularized.fit(X_train_poly, y_train)\n",
    "    y_pred_regularized = model_regularized.predict(X_test_poly)\n",
    "    predictions_on_test_regularized.append(y_pred_regularized)\n",
    "\n",
    "predictions_on_test_unregularized = np.array(predictions_on_test_unregularized)\n",
    "predictions_on_test_regularized = np.array(predictions_on_test_regularized)\n",
    "\n",
    "mean_prediction_unregularized = np.mean(predictions_on_test_unregularized, axis=0)\n",
    "mean_prediction_regularized = np.mean(predictions_on_test_regularized, axis=0)\n",
    "\n",
    "actual_y_test = true_function(x_test)\n",
    "\n",
    "# Squared Bias\n",
    "squared_bias_unregularized = np.mean((mean_prediction_unregularized - actual_y_test) ** 2)\n",
    "squared_bias_regularized = np.mean((mean_prediction_regularized - actual_y_test) ** 2)\n",
    "\n",
    "# Variance\n",
    "variance_unregularized = np.mean(np.var(predictions_on_test_unregularized, axis=0))\n",
    "variance_regularized = np.mean(np.var(predictions_on_test_regularized, axis=0))\n",
    "\n",
    "# MSE\n",
    "mse_unregularized = squared_bias_unregularized + variance_unregularized + noise_std ** 2\n",
    "mse_regularized = squared_bias_regularized + variance_regularized + noise_std ** 2\n",
    "\n",
    "# Display results\n",
    "print(f\"Unregularized Model (Degree 10):\")\n",
    "print(f\"Squared Bias: {squared_bias_unregularized:.4f}\")\n",
    "print(f\"Variance: {variance_unregularized:.4f}\")\n",
    "print(f\"MSE: {mse_unregularized:.4f}\")\n",
    "print(\"\\nRegularized Model (Degree 10 with L2):\")\n",
    "print(f\"Squared Bias: {squared_bias_regularized:.4f}\")\n",
    "print(f\"Variance: {variance_regularized:.4f}\")\n",
    "print(f\"MSE: {mse_regularized:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2-2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "data = fetch_openml(data_id=31, as_frame=True) \n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Preprocess\n",
    "# Convert target to binary labels\n",
    "y = y.map({'good': 1, 'bad': 0})\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "numeric_features = X.select_dtypes(include=['number']).columns\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "X_categorical_encoded = encoder.fit_transform(X[categorical_features])\n",
    "X_numeric = X[numeric_features].values\n",
    "X_encoded = np.hstack((X_categorical_encoded, X_numeric))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Use Adaboost and Logistic Regression\n",
    "clf_adaboost = AdaBoostClassifier(random_state=42)\n",
    "clf_logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf_adaboost.fit(X_train, y_train)\n",
    "clf_logreg.fit(X_train, y_train)\n",
    "\n",
    "y_score_adaboost = clf_adaboost.predict_proba(X_test)[:, 1]\n",
    "y_score_logreg = clf_logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC curve and PR curve for each classifier\n",
    "fpr_adaboost, tpr_adaboost, _ = roc_curve(y_test, y_score_adaboost)\n",
    "fpr_logreg, tpr_logreg, _ = roc_curve(y_test, y_score_logreg)\n",
    "\n",
    "precision_adaboost, recall_adaboost, _ = precision_recall_curve(y_test, y_score_adaboost)\n",
    "precision_logreg, recall_logreg, _ = precision_recall_curve(y_test, y_score_logreg)\n",
    "\n",
    "all_positive_tpr = 1\n",
    "all_positive_fpr = 1\n",
    "positive_proportion = np.sum(y_test.to_numpy().astype(int)) / len(y_test)  # Proportion of positives in test set\n",
    "all_positive_precision = positive_proportion\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(fpr_adaboost, tpr_adaboost, label='Adaboost')\n",
    "plt.plot(fpr_logreg, tpr_logreg, label='Logistic Regression')\n",
    "plt.scatter(all_positive_fpr, all_positive_tpr, color='red', label='All Positive Classifier', marker='x')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot PR curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(recall_adaboost, precision_adaboost, label='Adaboost')\n",
    "plt.plot(recall_logreg, precision_logreg, label='Logistic Regression')\n",
    "plt.scatter(1, all_positive_precision, color='red', label='All Positive Classifier', marker='x')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "auroc_adaboost = roc_auc_score(y_test, y_score_adaboost)\n",
    "auroc_logreg = roc_auc_score(y_test, y_score_logreg)\n",
    "\n",
    "aupr_adaboost = average_precision_score(y_test, y_score_adaboost)\n",
    "aupr_logreg = average_precision_score(y_test, y_score_logreg)\n",
    "\n",
    "def calculate_auprg(precision, recall, positive_proportion):\n",
    "    precision_gain = (precision - positive_proportion) / (1 - positive_proportion)\n",
    "    recall_gain = (recall - positive_proportion) / (1 - positive_proportion)\n",
    "    return np.trapz(precision_gain, recall_gain)\n",
    "\n",
    "poss = positive_proportion\n",
    "\n",
    "# Calculate AUPRG for both classifiers\n",
    "auprg_adaboost = calculate_auprg(precision_adaboost, recall_adaboost, poss)\n",
    "auprg_logreg = calculate_auprg(precision_logreg, recall_logreg, poss)\n",
    "\n",
    "\n",
    "print(f\"AUROC (Adaboost): {auroc_adaboost:.3f}\")\n",
    "print(f\"AUROC (Logistic Regression): {auroc_logreg:.3f}\")\n",
    "print(f\"AUPR (Adaboost): {aupr_adaboost:.3f}\")\n",
    "print(f\"AUPR (Logistic Regression): {aupr_logreg:.3f}\")\n",
    "print(f\"AUPRG (Adaboost): {auprg_adaboost:.3f}\")\n",
    "print(f\"AUPRG (Logistic Regression): {auprg_logreg:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
